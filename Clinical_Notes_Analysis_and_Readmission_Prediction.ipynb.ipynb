{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modelling for Patient Readmission by extraction and analysis of High-granularity data from notes\n",
    "## Project Repository\n",
    "\n",
    "You can view the full code and dataset on GitHub: [GitHub Repository Link](https://github.com/Jyotheekiran/Patient-Readmission/)\n",
    "\n",
    "\n",
    "### Installing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (4.44.2)\n",
      "Requirement already satisfied: torch in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (2.3.1)\n",
      "Requirement already satisfied: seaborn in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: shap in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (0.44.1)\n",
      "Requirement already satisfied: lime in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (0.2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers) (0.23.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib) (6.1.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: slicer==0.0.7 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: numba in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from shap) (0.58.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from shap) (3.0.0)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from lime) (0.21.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.11.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from scikit-image>=0.12->lime) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from scikit-image>=0.12->lime) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from scikit-image>=0.12->lime) (1.4.1)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from scikit-image>=0.12->lime) (0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from numba->shap) (0.41.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from numba->shap) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from sympy->torch) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (0.34.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers[torch]) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers[torch]) (0.23.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers[torch]) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: torch in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from torch->transformers[torch]) (1.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from torch->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from torch->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from torch->transformers[torch]) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers[torch]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers[torch]) (2023.11.17)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->transformers[torch]) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->transformers[torch]) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "   ---------------------------------------- 0.0/324.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/324.4 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 143.4/324.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  317.4/324.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 324.4/324.4 kB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.34.0\n",
      "    Uninstalling accelerate-0.34.0:\n",
      "      Successfully uninstalled accelerate-0.34.0\n",
      "Successfully installed accelerate-0.34.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts accelerate-config.exe, accelerate-estimate-memory.exe, accelerate-launch.exe, accelerate-merge-weights.exe and accelerate.exe are installed in 'C:\\Users\\mjoth\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imblearn in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from imblearn) (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\programdata\\anaconda3\\envs\\datascience\\lib\\site-packages (from imbalanced-learn->imblearn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from imbalanced-learn->imblearn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mjoth\\appdata\\roaming\\python\\python38\\site-packages (from imbalanced-learn->imblearn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install Required Libraries\n",
    "!pip3 install transformers torch seaborn matplotlib scikit-learn pandas shap lime\n",
    "!pip install transformers[torch] accelerate -U\n",
    "!pip install imblearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Import Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection & Preprocessing\n",
    "Use the MIMIC-III dataset and perform text cleaning and tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjoth\\AppData\\Local\\Temp\\ipykernel_24044\\1247399444.py:2: DtypeWarning: Columns (0,1,2,8,9,11,12,13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  noteevents = pd.read_csv('C:/Users/mjoth/DataScience/DissertationProject/GitHub/Patient-Readmission/data/NOTEEVENTS.csv')\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load Data\n",
    "noteevents = pd.read_csv('C:/Users/mjoth/DataScience/DissertationProject/GitHub/Patient-Readmission/data/NOTEEVENTS.csv')\n",
    "diagnoses = pd.read_csv('C:/Users/mjoth/DataScience/DissertationProject/GitHub/Patient-Readmission/data/DIAGNOSES_ICD.csv')\n",
    "procedures = pd.read_csv('C:/Users/mjoth/DataScience/DissertationProject/GitHub/Patient-Readmission/data/PROCEDURES_ICD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjoth\\AppData\\Local\\Temp\\ipykernel_24044\\4013269040.py:5: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(pd.read_csv('C:/Users/mjoth/DataScience/DissertationProject/GitHub/Patient-Readmission/data/NOTEEVENTS.csv', chunksize=chunksize)):\n",
      "C:\\Users\\mjoth\\AppData\\Local\\Temp\\ipykernel_24044\\4013269040.py:5: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(pd.read_csv('C:/Users/mjoth/DataScience/DissertationProject/GitHub/Patient-Readmission/data/NOTEEVENTS.csv', chunksize=chunksize)):\n",
      "C:\\Users\\mjoth\\AppData\\Local\\Temp\\ipykernel_24044\\4013269040.py:5: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(pd.read_csv('C:/Users/mjoth/DataScience/DissertationProject/GitHub/Patient-Readmission/data/NOTEEVENTS.csv', chunksize=chunksize)):\n",
      "C:\\Users\\mjoth\\AppData\\Local\\Temp\\ipykernel_24044\\4013269040.py:5: DtypeWarning: Columns (0,1,2,8,9,11,12,13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(pd.read_csv('C:/Users/mjoth/DataScience/DissertationProject/GitHub/Patient-Readmission/data/NOTEEVENTS.csv', chunksize=chunksize)):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the large file in chunks\n",
    "chunksize = 100000  # Set chunk size (adjust based on your file)\n",
    "for i, chunk in enumerate(pd.read_csv('C:/Users/mjoth/DataScience/DissertationProject/GitHub/Patient-Readmission/data/NOTEEVENTS.csv', chunksize=chunksize)):\n",
    "    chunk.to_csv(f'noteevents_part_{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1678764</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>17/07/2138</td>\n",
       "      <td>17/07/2138 22:51</td>\n",
       "      <td>17/07/2138 23:12</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16929.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neonatology Attending Triage Note\\n\\nBaby [**N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1678765</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>17/07/2138</td>\n",
       "      <td>17/07/2138 23:08</td>\n",
       "      <td>17/07/2138 23:18</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>17774.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nursing Transfer note\\n\\n\\nPt admitted to NICU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272794</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/10/2101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECG</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sinus rhythm\\nInferior/lateral ST-T changes ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>769224</td>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>26/10/2101</td>\n",
       "      <td>26/10/2101 06:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>272793</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/10/2101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECG</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sinus rhythm\\nA-V delay\\nNonspecific inferior ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROW_ID SUBJECT_ID HADM_ID   CHARTDATE         CHARTTIME         STORETIME  \\\n",
       "0  1678764          2  163353  17/07/2138  17/07/2138 22:51  17/07/2138 23:12   \n",
       "1  1678765          2  163353  17/07/2138  17/07/2138 23:08  17/07/2138 23:18   \n",
       "2   272794          3     NaN  06/10/2101               NaN               NaN   \n",
       "3   769224          3  145834  26/10/2101  26/10/2101 06:01               NaN   \n",
       "4   272793          3     NaN  11/10/2101               NaN               NaN   \n",
       "\n",
       "        CATEGORY          DESCRIPTION     CGID ISERROR  \\\n",
       "0  Nursing/other               Report  16929.0     NaN   \n",
       "1  Nursing/other               Report  17774.0     NaN   \n",
       "2            ECG               Report      NaN     NaN   \n",
       "3      Radiology  CHEST (PORTABLE AP)      NaN     NaN   \n",
       "4            ECG               Report      NaN     NaN   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Neonatology Attending Triage Note\\n\\nBaby [**N...  \n",
       "1  Nursing Transfer note\\n\\n\\nPt admitted to NICU...  \n",
       "2  Sinus rhythm\\nInferior/lateral ST-T changes ar...  \n",
       "3  [**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...  \n",
       "4  Sinus rhythm\\nA-V delay\\nNonspecific inferior ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Data Exploration\n",
    "noteevents = noteevents.loc[:, ~noteevents.columns.str.contains('^Unnamed')]\n",
    "noteevents.head()  # Display the first few rows of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>seq_num</th>\n",
       "      <th>icd9_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112344</td>\n",
       "      <td>10006</td>\n",
       "      <td>142345</td>\n",
       "      <td>1</td>\n",
       "      <td>99591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112345</td>\n",
       "      <td>10006</td>\n",
       "      <td>142345</td>\n",
       "      <td>2</td>\n",
       "      <td>99662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112346</td>\n",
       "      <td>10006</td>\n",
       "      <td>142345</td>\n",
       "      <td>3</td>\n",
       "      <td>5672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112347</td>\n",
       "      <td>10006</td>\n",
       "      <td>142345</td>\n",
       "      <td>4</td>\n",
       "      <td>40391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112348</td>\n",
       "      <td>10006</td>\n",
       "      <td>142345</td>\n",
       "      <td>5</td>\n",
       "      <td>42731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  subject_id  hadm_id  seq_num icd9_code\n",
       "0  112344       10006   142345        1     99591\n",
       "1  112345       10006   142345        2     99662\n",
       "2  112346       10006   142345        3      5672\n",
       "3  112347       10006   142345        4     40391\n",
       "4  112348       10006   142345        5     42731"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnoses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>seq_num</th>\n",
       "      <th>icd9_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3994</td>\n",
       "      <td>10114</td>\n",
       "      <td>167957</td>\n",
       "      <td>1</td>\n",
       "      <td>3605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3995</td>\n",
       "      <td>10114</td>\n",
       "      <td>167957</td>\n",
       "      <td>2</td>\n",
       "      <td>3722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3996</td>\n",
       "      <td>10114</td>\n",
       "      <td>167957</td>\n",
       "      <td>3</td>\n",
       "      <td>8856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3997</td>\n",
       "      <td>10114</td>\n",
       "      <td>167957</td>\n",
       "      <td>4</td>\n",
       "      <td>9920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3998</td>\n",
       "      <td>10114</td>\n",
       "      <td>167957</td>\n",
       "      <td>5</td>\n",
       "      <td>9671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  subject_id  hadm_id  seq_num  icd9_code\n",
       "0    3994       10114   167957        1       3605\n",
       "1    3995       10114   167957        2       3722\n",
       "2    3996       10114   167957        3       8856\n",
       "3    3997       10114   167957        4       9920\n",
       "4    3998       10114   167957        5       9671"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procedures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROW_ID           1475\n",
       "SUBJECT_ID       5621\n",
       "HADM_ID         54676\n",
       "CHARTDATE        6600\n",
       "CHARTTIME       72631\n",
       "STORETIME      177545\n",
       "CATEGORY         6745\n",
       "DESCRIPTION      6753\n",
       "CGID           177580\n",
       "ISERROR        489250\n",
       "TEXT             6765\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "noteevents.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6765"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noteevents['TEXT'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noteevents['TEXT_LENGTH'] = noteevents['TEXT'].fillna('').apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noteevents = noteevents.dropna(subset=['TEXT'])\n",
    "noteevents['TEXT'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mjoth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove any text within square brackets (like anonymized info)\n",
    "    text = re.sub(r'\\n', ' ', text)  # Remove newlines\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)  # Remove words with numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = text.split()  # Tokenize text\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply cleaning to the 'TEXT' column\n",
    "noteevents['CLEAN_TEXT'] = noteevents['TEXT'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>TEXT_LENGTH</th>\n",
       "      <th>CLEAN_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1678764</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>17/07/2138</td>\n",
       "      <td>17/07/2138 22:51</td>\n",
       "      <td>17/07/2138 23:12</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16929.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neonatology Attending Triage Note\\n\\nBaby [**N...</td>\n",
       "      <td>1296</td>\n",
       "      <td>neonatology attending triage note baby term ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1678765</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>17/07/2138</td>\n",
       "      <td>17/07/2138 23:08</td>\n",
       "      <td>17/07/2138 23:18</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>17774.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nursing Transfer note\\n\\n\\nPt admitted to NICU...</td>\n",
       "      <td>522</td>\n",
       "      <td>nursing transfer note pt admitted nicu sepsis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272794</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/10/2101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECG</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sinus rhythm\\nInferior/lateral ST-T changes ar...</td>\n",
       "      <td>162</td>\n",
       "      <td>sinus rhythm inferiorlateral stt changes nonsp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>769224</td>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>26/10/2101</td>\n",
       "      <td>26/10/2101 06:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...</td>\n",
       "      <td>1310</td>\n",
       "      <td>chest portable ap clip reason ro infiltrate __...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>272793</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/10/2101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECG</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sinus rhythm\\nA-V delay\\nNonspecific inferior ...</td>\n",
       "      <td>129</td>\n",
       "      <td>sinus rhythm av delay nonspecific inferior wav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROW_ID SUBJECT_ID HADM_ID   CHARTDATE         CHARTTIME         STORETIME  \\\n",
       "0  1678764          2  163353  17/07/2138  17/07/2138 22:51  17/07/2138 23:12   \n",
       "1  1678765          2  163353  17/07/2138  17/07/2138 23:08  17/07/2138 23:18   \n",
       "2   272794          3     NaN  06/10/2101               NaN               NaN   \n",
       "3   769224          3  145834  26/10/2101  26/10/2101 06:01               NaN   \n",
       "4   272793          3     NaN  11/10/2101               NaN               NaN   \n",
       "\n",
       "        CATEGORY          DESCRIPTION     CGID ISERROR  \\\n",
       "0  Nursing/other               Report  16929.0     NaN   \n",
       "1  Nursing/other               Report  17774.0     NaN   \n",
       "2            ECG               Report      NaN     NaN   \n",
       "3      Radiology  CHEST (PORTABLE AP)      NaN     NaN   \n",
       "4            ECG               Report      NaN     NaN   \n",
       "\n",
       "                                                TEXT  TEXT_LENGTH  \\\n",
       "0  Neonatology Attending Triage Note\\n\\nBaby [**N...         1296   \n",
       "1  Nursing Transfer note\\n\\n\\nPt admitted to NICU...          522   \n",
       "2  Sinus rhythm\\nInferior/lateral ST-T changes ar...          162   \n",
       "3  [**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...         1310   \n",
       "4  Sinus rhythm\\nA-V delay\\nNonspecific inferior ...          129   \n",
       "\n",
       "                                          CLEAN_TEXT  \n",
       "0  neonatology attending triage note baby term ma...  \n",
       "1  nursing transfer note pt admitted nicu sepsis ...  \n",
       "2  sinus rhythm inferiorlateral stt changes nonsp...  \n",
       "3  chest portable ap clip reason ro infiltrate __...  \n",
       "4  sinus rhythm av delay nonspecific inferior wav...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noteevents.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Note Length Distribution\n",
    "noteevents['TEXT_LENGTH'] = noteevents['TEXT'].apply(len)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(noteevents['TEXT_LENGTH'], bins=50)\n",
    "plt.title('Distribution of Note Lengths')\n",
    "plt.xlabel('Length of Notes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph represents the distribution of note lengths from the noteevents dataset, where each note's length is calculated using the number of characters in the TEXT field of the dataset. \n",
    "Most Notes Are Short: The majority of notes have a shorter length, concentrated around 1000 characters or fewer.\n",
    "Few Very Long Notes: Only a small number of notes are extremely long, stretching towards 30,000 characters, but they are rare compared to the shorter notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Model Development\n",
    "Develop a BERT baseline model and fine-tune BioBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjoth\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocessing function for tokenizing texts\n",
    "def preprocess_data(texts, max_length=128):\n",
    "    # Ensure texts is a list (for batch processing)\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]  # Convert single string into a list\n",
    "    \n",
    "    # Tokenize with padding, truncation, and attention masks\n",
    "    return tokenizer(\n",
    "        texts, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        return_tensors='pt',  # Return as PyTorch tensors\n",
    "        return_attention_mask=True  # Return attention mask for padded tokens\n",
    "    )\n",
    "\n",
    "# Limit the dataset to 1000 rows\n",
    "noteevents_small = noteevents.head(1000)\n",
    "\n",
    "# Ensure 'TEXT' column is a list of strings\n",
    "texts = noteevents_small['TEXT'].tolist()  # Convert Pandas Series to a list of strings\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_train_data = preprocess_data(texts)  # Now texts is a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjoth\\AppData\\Local\\Temp\\ipykernel_24044\\392128260.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  noteevents_small.loc[:, 'LABEL'] = [1 if 'diagnosis' in text.lower() else 0 for text in noteevents_small['TEXT']]\n"
     ]
    }
   ],
   "source": [
    "noteevents_small.loc[:, 'LABEL'] = [1 if 'diagnosis' in text.lower() else 0 for text in noteevents_small['TEXT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code is adding a new column called LABEL to the noteevents_small DataFrame. This line of code creates a binary label (1 or 0) based on whether the word \"diagnosis\" is found in the text of the clinical note.\n",
    "If the word \"diagnosis\" is present in the text, the label for that row will be 1.\n",
    "If the word \"diagnosis\" is not present, the label will be 0.\n",
    "\n",
    "This line of code creates a new binary LABEL column in the DataFrame, where each row is labeled with 1 if the word \"diagnosis\" is found in the TEXT field and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into training and test sets so that you can evaluate the model's performance on unseen data. The training data is used to fine-tune the model, while the test data is reserved for evaluating how well the model generalizes to new data (i.e., data it hasn't seen during training).\n",
    "We are splitting the dataset into 2 parts such that the training dataset will be 80% and test dataset will be 20% of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tokenize the text data\n",
    "texts = noteevents_small['TEXT'].tolist()\n",
    "tokenized_data = preprocess_data(texts)\n",
    "\n",
    "# Get input_ids and attention masks\n",
    "input_ids = tokenized_data['input_ids']\n",
    "attention_masks = tokenized_data['attention_mask']\n",
    "\n",
    "# Perform the train-test split ensuring alignment between input_ids and attention_mask\n",
    "X_train, X_test, y_train, y_test, train_attention_mask, test_attention_mask = train_test_split(\n",
    "    input_ids, \n",
    "    noteevents_small['LABEL'], \n",
    "    attention_masks, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to tensors\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, train_attention_mask, y_train_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, test_attention_mask, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 128])\n",
      "torch.Size([800, 128])\n",
      "torch.Size([800])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)             \n",
    "print(train_attention_mask.shape) \n",
    "print(y_train_tensor.shape)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "# Define the data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# Define model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjoth\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56382df09a0b4c3a8681839f927984b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROW_ID': '1678764', 'SUBJECT_ID': '2', 'HADM_ID': '163353', 'CHARTDATE': '17/07/2138', 'CHARTTIME': '17/07/2138 22:51', 'STORETIME': '17/07/2138 23:12', 'CATEGORY': 'Nursing/other', 'DESCRIPTION': 'Report', 'CGID': 16929.0, 'ISERROR': None, 'TEXT_LENGTH': 1296, 'CLEAN_TEXT': 'neonatology attending triage note baby term male infant admitted nicu sepsis evaluation asked evaluate baby dr mother years old pns pos ab neg hbsag neg rpr nr ri gbs neg pregnancy uncomplicated delivery csection failure progress apgars mother treated antibiotics maternal temp prior delivery mothers temp lower hours rose pe baby vigorous aga vs hr rr bp sat ra heent af soft flat molding noted palate intact resp breath sounds clear equal cvs normal high pitched systolic murmur lsb abd soft normal bowel sounds organomegaly gu normal male testes descended bilaterally neuro tone wnl symmetrical exam ds assessmentplan term male infant increased risk sepsis check cbc diff plats blood culture cover antibiotics least hours pending results cultures work possible lp culture positive clinical signs sepsis develop', 'labels': 0, '__index_level_0__': 0, 'input_ids': [101, 16231, 10610, 6483, 7052, 13012, 4270, 3602, 3336, 1031, 1008, 1008, 2171, 1006, 9152, 1007, 1015, 1008, 1008, 1033, 1031, 1008, 1008, 2124, 2197, 18442, 1016, 1008, 1008, 1033, 2003, 1037, 2744, 3287, 10527, 4914, 2000, 1996, 27969, 2226, 2005, 19802, 6190, 9312, 1012, 2356, 2000, 16157, 3336, 2011, 2852, 1012, 1031, 1008, 1008, 2034, 2171, 2549, 1006, 2171, 4502, 12079, 2078, 2487, 1007, 1017, 1008, 1008, 1033, 1031, 1008, 1008, 2197, 2171, 1006, 2171, 4502, 12079, 2078, 2487, 1007, 1018, 1008, 1008, 1033, 1012, 2388, 2003, 4090, 2086, 2214, 1043, 2487, 1052, 2692, 1011, 1015, 1012, 1052, 3619, 1024, 1037, 13433, 2015, 1010, 11113, 11265, 2290, 1010, 1044, 5910, 8490, 11265, 2290, 1010, 1054, 18098, 17212, 1010, 15544, 1010, 16351, 2015, 11265, 2290, 1012, 10032, 2001, 4895, 9006, 24759, 17872, 1012, 6959, 2001, 2011, 1039, 1011, 2930, 2044, 4945, 2000, 5082, 1012, 9706, 6843, 2015, 1023, 1010, 1023, 1012, 2388, 2001, 5845, 2007, 24479, 2138, 1997, 11062, 8915, 8737, 1997, 2531, 1012, 1017, 2074, 3188, 2000, 6959, 1012, 2388, 1005, 1055, 8915, 8737, 2001, 2059, 2896, 2021, 2012, 1016, 2847, 3123, 2153, 2000, 9402, 1012, 21877, 1011, 3336, 2003, 1031, 1008, 1008, 2171, 2475, 1006, 9152, 1007, 1019, 1008, 1008, 1033, 1998, 21813, 1010, 12943, 2050, 1012, 5443, 1011, 1056, 5818, 1012, 1019, 17850, 13741, 25269, 4229, 17531, 5824, 1013, 3486, 4466, 1010, 1051, 2475, 2938, 2531, 1003, 1999, 10958, 18235, 3372, 1011, 21358, 3730, 1998, 4257, 1010, 2070, 18282, 2075, 3264, 1012, 14412, 3686, 10109, 1012, 24501, 2361, 1011, 3052, 4165, 3154, 1998, 5020, 1012, 26226, 2015, 1011, 1055, 2487, 1055, 2475, 3671, 1010, 1031, 1008, 1008, 1017, 1011, 2324, 1008, 1008, 1033, 2152, 8219, 25353, 16033, 10415, 20227, 2012, 1048, 19022, 19935, 1011, 3730, 2007, 3671, 6812, 2884, 4165, 1010, 2053, 5812, 8462, 9692, 2100, 19739, 1011, 3671, 3287, 2007, 3231, 2229, 9287, 17758, 2135, 11265, 10976, 1011, 4309, 1059, 20554, 1010, 23476, 11360, 16233, 1011, 5824, 7667, 1013, 2933, 1024, 2744, 3287, 10527, 2007, 3445, 3891, 1997, 19802, 6190, 1012, 2097, 4638, 13581, 4487, 4246, 1998, 20228, 11149, 1010, 2668, 3226, 1012, 2097, 3104, 2007, 24479, 2005, 2012, 2560, 4466, 2847, 14223, 3463, 1997, 8578, 1012, 2582, 2147, 2039, 2007, 2825, 6948, 2065, 3226, 2003, 3893, 2030, 6612, 5751, 1997, 19802, 6190, 4503, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Assume noteevents_small is a Pandas DataFrame\n",
    "dataset = Dataset.from_pandas(noteevents_small)\n",
    "\n",
    "# Tokenize the dataset and ensure it returns `input_ids`, `attention_mask`, and `labels`\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['TEXT'], padding='max_length', truncation=True)\n",
    "\n",
    "# Tokenizing the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unneeded columns (we only need input_ids, attention_mask, and labels)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['TEXT'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column('LABEL', 'labels')\n",
    "\n",
    "# Check if the dataset returns the correct format\n",
    "tokenized_dataset[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits to predicted class labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Calculate precision, recall, F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    \n",
    "    # Calculate AUC-ROC score\n",
    "    # We assume binary classification where the second class is the positive class\n",
    "    probs = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)\n",
    "    auc = roc_auc_score(labels, probs[:, 1])\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ec43744d94494ea31fa442cec6cfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eff1f0afd134979831f708e2a6f62cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22330987453460693, 'eval_accuracy': 0.87, 'eval_precision': 0.6, 'eval_recall': 1.0, 'eval_f1': 0.7499999999999999, 'eval_auc': 0.9369326325848065, 'eval_runtime': 216.7071, 'eval_samples_per_second': 0.923, 'eval_steps_per_second': 0.06, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    save_safetensors=False\n",
    ")\n",
    "\n",
    "# Set up the Trainer with the properly formatted dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics, # Automatically handle padding and formatting\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13452335a0ec42fdb7cbfc24e1bf4aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2266383320093155,\n",
       " 'eval_runtime': 232.7334,\n",
       " 'eval_samples_per_second': 0.859,\n",
       " 'eval_steps_per_second': 0.056,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results_bert = trainer.evaluate(eval_dataset=test_dataset)\n",
    "eval_results_bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d7758e097a40d59e7d5b238107a5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61, Precision: 0.21153846153846154, Recall: 0.22916666666666666, F1-Score: 0.21999999999999997, AUC-ROC: 0.4797149122807018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "# Evaluation on test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, preds, average='binary')\n",
    "roc_auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}, AUC-ROC: {roc_auc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy (61%): The model correctly predicts the outcome 61% of the time, which is slightly better than random guessing (50%).\n",
    "\n",
    "Precision (17%): Only 17% of the instances the model predicted as positive were actually positive. This indicates a high rate of false positives.\n",
    "\n",
    "Recall (17%): The model correctly identified 17% of all actual positive instances, meaning it missed a lot of true positives (high false negatives).\n",
    "\n",
    "F1-Score (17%): This low score reflects poor balance between precision and recall, indicating the model is not effective at identifying the positive class.\n",
    "\n",
    "AUC-ROC (46%): The models ability to distinguish between the positive and negative classes is slightly worse than random guessing (which would be 50%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BioBERT Model and Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjoth\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load BioBERT model and tokenizer\n",
    "tokenizer_biobert = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "model_biobert = BertForSequenceClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.1', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Assuming your DataFrame is called `noteevents_small`\n",
    "noteevents_dataset = Dataset.from_pandas(noteevents_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aef241c6fa346b8857645ac7b1e8b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function_biobert(examples):\n",
    "    return tokenizer_biobert(examples['TEXT'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset_biobert = noteevents_dataset.map(tokenize_function_biobert, batched=True)\n",
    "tokenized_dataset_biobert = tokenized_dataset_biobert.remove_columns(['TEXT'])\n",
    "tokenized_dataset_biobert = tokenized_dataset_biobert.rename_column('LABEL', 'labels')\n",
    "\n",
    "# Split into train and test datasets\n",
    "train_test_split_biobert = tokenized_dataset_biobert.train_test_split(test_size=0.2)\n",
    "train_dataset_biobert = train_test_split_biobert['train']\n",
    "test_dataset_biobert = train_test_split_biobert['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Define the data collator for BioBERT\n",
    "data_collator_biobert = DataCollatorWithPadding(tokenizer=tokenizer_biobert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 3: Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd51fcdb463451198c3aea6679b31e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bf8666b8184e288fcb009bb08e4716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0794716626405716, 'eval_accuracy': 0.985, 'eval_precision': 0.9777777777777777, 'eval_recall': 0.9565217391304348, 'eval_f1': 0.967032967032967, 'eval_auc': 0.9905420666290232, 'eval_runtime': 31.0535, 'eval_samples_per_second': 6.441, 'eval_steps_per_second': 0.419, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c669df950649a5ac5a894d641c5b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07207995653152466, 'eval_accuracy': 0.985, 'eval_precision': 0.9777777777777777, 'eval_recall': 0.9565217391304348, 'eval_f1': 0.967032967032967, 'eval_auc': 0.996188594014681, 'eval_runtime': 30.7241, 'eval_samples_per_second': 6.51, 'eval_steps_per_second': 0.423, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0830b76426be4e85add189cbe836ec26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07194579392671585, 'eval_accuracy': 0.985, 'eval_precision': 0.9777777777777777, 'eval_recall': 0.9565217391304348, 'eval_f1': 0.967032967032967, 'eval_auc': 0.9963297571993224, 'eval_runtime': 1114.3986, 'eval_samples_per_second': 0.179, 'eval_steps_per_second': 0.012, 'epoch': 3.0}\n",
      "{'train_runtime': 2810.946, 'train_samples_per_second': 0.854, 'train_steps_per_second': 0.107, 'train_loss': 0.06689790089925131, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.06689790089925131, metrics={'train_runtime': 2810.946, 'train_samples_per_second': 0.854, 'train_steps_per_second': 0.107, 'total_flos': 157866633216000.0, 'train_loss': 0.06689790089925131, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments and disable safetensors saving\n",
    "training_args_biobert = TrainingArguments(\n",
    "    output_dir='./results_biobert',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir='./logs_biobert',\n",
    "    save_safetensors=False  # Disable safetensors saving\n",
    ")\n",
    "\n",
    "# Custom Trainer that ensures model tensors are contiguous before saving\n",
    "class ContiguousTrainer(Trainer):\n",
    "    def _save(self, output_dir: str, state_dict=None):\n",
    "        # Make all model parameters contiguous before saving\n",
    "        for param in self.model.parameters():\n",
    "            param.data = param.data.contiguous()\n",
    "        # Save the model using the regular Trainer save method\n",
    "        super()._save(output_dir, state_dict)\n",
    "\n",
    "# Initialize the custom Trainer\n",
    "trainer_biobert = ContiguousTrainer(\n",
    "    model=model_biobert,\n",
    "    args=training_args_biobert,\n",
    "    train_dataset=train_dataset_biobert,\n",
    "    eval_dataset=test_dataset_biobert,\n",
    "    data_collator=data_collator_biobert,\n",
    "    compute_metrics=compute_metrics,  # Reuse the metrics function\n",
    ")\n",
    "\n",
    "# Train BioBERT\n",
    "trainer_biobert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_biobert\\\\tokenizer_config.json',\n",
       " './fine_tuned_biobert\\\\special_tokens_map.json',\n",
       " './fine_tuned_biobert\\\\vocab.txt',\n",
       " './fine_tuned_biobert\\\\added_tokens.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model and tokenizer\n",
    "trainer_biobert.save_model('./fine_tuned_biobert')\n",
    "tokenizer_biobert.save_pretrained('./fine_tuned_biobert')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Model Evaluation\n",
    "Evaluate the performance of the model using Accuracy, Precision, F1-score, and AUC-ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 28996\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer_biobert.vocab_size\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b0e275c39b4572bd68c2107f2f1020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012500911019742489, 'eval_accuracy': 0.99375, 'eval_precision': 0.9775280898876404, 'eval_recall': 0.9942857142857143, 'eval_f1': 0.9858356940509916, 'eval_auc': 0.9999177142857143, 'eval_runtime': 127.4977, 'eval_samples_per_second': 6.275, 'eval_steps_per_second': 0.392, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results_biobert = trainer_biobert.evaluate(eval_dataset=train_dataset_biobert)\n",
    "eval_results_biobert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Accuracy (96.5%):** The model correctly classifies 96.5% of the samples, which is excellent.<br>\n",
    "**Precision (91.7%):** When the model predicts a positive class, it's correct 91.7% of the time, showing a low rate of false positives.<br/>\n",
    "**Recall (93.6%):** The model identifies 93.6% of actual positive cases, meaning it's catching most of the true positives with few false negatives. <br/>\n",
    "**F1 Score (92.6%):** The model balances precision and recall effectively, resulting in a high overall performance.<br/>\n",
    "**AUC-ROC (98.8%):** The model is highly capable of distinguishing between positive and negative classes.<br/>\n",
    "**Loss (0.1111):** The model's predictions are very close to the true labels, indicating strong performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5165372d23b54f319e7cd91ad5a43ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.07194579392671585,\n",
       " 'eval_accuracy': 0.985,\n",
       " 'eval_precision': 0.9777777777777777,\n",
       " 'eval_recall': 0.9565217391304348,\n",
       " 'eval_f1': 0.967032967032967,\n",
       " 'eval_auc': 0.9963297571993224,\n",
       " 'eval_runtime': 32.9095,\n",
       " 'eval_samples_per_second': 6.077,\n",
       " 'eval_steps_per_second': 0.395,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results_biobert_test = trainer_biobert.evaluate(eval_dataset=test_dataset_biobert)\n",
    "eval_results_biobert_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save training results\n",
    "with open(\"train_results.json\", \"w\") as f:\n",
    "    json.dump(eval_results_biobert, f)\n",
    "\n",
    "# Save test results\n",
    "with open(\"test_results.json\", \"w\") as f:\n",
    "    json.dump(eval_results_biobert_test, f)\n",
    "\n",
    "# Save the evaluation results to a text file\n",
    "with open('biobert_evaluation_results_test.txt', 'w') as f:\n",
    "    for key, value in eval_results_biobert_test.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset Evaluation:\n",
    "\n",
    "**Accuracy:** 98.5% - The model correctly predicted the labels for 98.5% of the test data, which is slightly lower than the training accuracy but still very high.\n",
    "\n",
    "**Precision:** 97.8% - All positive predictions on the test data were correct.\n",
    "\n",
    "**Recall:** 95.7% - The model correctly identified 93.62% of all actual positive cases on the test data.\n",
    "\n",
    "**F1-Score:** 96.70% - The F1-Score on the test data remains high, showing that the model generalizes well.\n",
    "\n",
    "**AUC:** 99.6% - The model continues to distinguish effectively between classes on unseen data.\n",
    "\n",
    "**Loss:** 0.072 - The slightly higher loss on the test data is expected and indicates minor generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('biobert_tokenizer\\\\tokenizer_config.json',\n",
       " 'biobert_tokenizer\\\\special_tokens_map.json',\n",
       " 'biobert_tokenizer\\\\vocab.txt',\n",
       " 'biobert_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_biobert.save_pretrained(\"biobert_model\")\n",
    "tokenizer_biobert.save_pretrained(\"biobert_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune BioBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjoth\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ff7369b87447c2a750ca5048668bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4cbaa7f90941c19707d761cae329f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06482462584972382, 'eval_accuracy': 0.99, 'eval_precision': 1.0, 'eval_recall': 0.9565217391304348, 'eval_f1': 0.9777777777777777, 'eval_auc': 0.9988706945228685, 'eval_runtime': 31.5431, 'eval_samples_per_second': 6.341, 'eval_steps_per_second': 0.412, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87058b85d3734915910c14beb8fde085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08945456147193909, 'eval_accuracy': 0.99, 'eval_precision': 1.0, 'eval_recall': 0.9565217391304348, 'eval_f1': 0.9777777777777777, 'eval_auc': 0.9964709203839639, 'eval_runtime': 30.5939, 'eval_samples_per_second': 6.537, 'eval_steps_per_second': 0.425, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da2f8f1432a4b16a6387bfa8986e0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09759984165430069, 'eval_accuracy': 0.985, 'eval_precision': 0.9777777777777777, 'eval_recall': 0.9565217391304348, 'eval_f1': 0.967032967032967, 'eval_auc': 0.9985883681535856, 'eval_runtime': 34.7507, 'eval_samples_per_second': 5.755, 'eval_steps_per_second': 0.374, 'epoch': 3.0}\n",
      "{'train_runtime': 1239.7822, 'train_samples_per_second': 1.936, 'train_steps_per_second': 0.242, 'train_loss': 0.01575436274210612, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.01575436274210612, metrics={'train_runtime': 1239.7822, 'train_samples_per_second': 1.936, 'train_steps_per_second': 0.242, 'total_flos': 157866633216000.0, 'train_loss': 0.01575436274210612, 'epoch': 3.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust training arguments if needed\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,  # Increase the number of epochs for further fine-tuning\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=2e-5  # Example of a lower learning rate for fine-tuning\n",
    ")\n",
    "\n",
    "# Re-train the model\n",
    "trainer_biobert = Trainer(\n",
    "    model=model_biobert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_biobert,\n",
    "    eval_dataset=test_dataset_biobert,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer_biobert.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07d662ee201490491a914e580657820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09759984165430069,\n",
       " 'eval_accuracy': 0.985,\n",
       " 'eval_precision': 0.9777777777777777,\n",
       " 'eval_recall': 0.9565217391304348,\n",
       " 'eval_f1': 0.967032967032967,\n",
       " 'eval_auc': 0.9985883681535856,\n",
       " 'eval_runtime': 32.0474,\n",
       " 'eval_samples_per_second': 6.241,\n",
       " 'eval_steps_per_second': 0.406,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results_biobert_finetuned = trainer_biobert.evaluate(eval_dataset=test_dataset_biobert)\n",
    "eval_results_biobert_finetuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./biobert_finetuned\\\\tokenizer_config.json',\n",
       " './biobert_finetuned\\\\special_tokens_map.json',\n",
       " './biobert_finetuned\\\\vocab.txt',\n",
       " './biobert_finetuned\\\\added_tokens.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model_biobert.save_pretrained(\"./biobert_finetuned\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_biobert.save_pretrained(\"./biobert_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_biobert = BertForSequenceClassification.from_pretrained(\"./biobert_finetuned\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_biobert = BertTokenizer.from_pretrained(\"./biobert_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjoth\\AppData\\Local\\Temp\\ipykernel_24044\\1004718125.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_noteevents['Readmission_Prediction'] = predictions.tolist()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Readmission_Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neonatology Attending Triage Note\\n\\nBaby [**N...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nursing Transfer note\\n\\n\\nPt admitted to NICU...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sinus rhythm\\nInferior/lateral ST-T changes ar...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sinus rhythm\\nA-V delay\\nNonspecific inferior ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>csru npn 7p-11p\\nPt is a/o x3, c/o pain and re...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Sinus bradycardia, rate 58. Probable left vent...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[**2133-1-9**] 11:46 AM\\n CTA CHEST W&amp;W/O C &amp;R...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[**2133-1-3**] 11:12 AM\\n CT CHEST W/CONTRAST;...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[**2133-1-16**] 8:34 AM\\n CT LUNG/MEDIASTINAL ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TEXT  Readmission_Prediction\n",
       "0    Neonatology Attending Triage Note\\n\\nBaby [**N...                     0.0\n",
       "1    Nursing Transfer note\\n\\n\\nPt admitted to NICU...                     0.0\n",
       "2    Sinus rhythm\\nInferior/lateral ST-T changes ar...                     0.0\n",
       "3    [**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...                     0.0\n",
       "4    Sinus rhythm\\nA-V delay\\nNonspecific inferior ...                     0.0\n",
       "..                                                 ...                     ...\n",
       "995  csru npn 7p-11p\\nPt is a/o x3, c/o pain and re...                     0.0\n",
       "996  Sinus bradycardia, rate 58. Probable left vent...                     0.0\n",
       "997  [**2133-1-9**] 11:46 AM\\n CTA CHEST W&W/O C &R...                     1.0\n",
       "998  [**2133-1-3**] 11:12 AM\\n CT CHEST W/CONTRAST;...                     1.0\n",
       "999  [**2133-1-16**] 8:34 AM\\n CT LUNG/MEDIASTINAL ...                     1.0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize only the first 1000 rows of the dataset\n",
    "subset_noteevents = noteevents.iloc[:1000]\n",
    "\n",
    "# Tokenize the text data\n",
    "inputs = tokenizer_biobert(subset_noteevents['TEXT'].tolist(), \n",
    "                           return_tensors=\"pt\", \n",
    "                           truncation=True, \n",
    "                           padding=True, \n",
    "                           max_length=128)\n",
    "\n",
    "# Model inference\n",
    "with torch.no_grad():\n",
    "    outputs = model_biobert(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Add predictions to the subset DataFrame\n",
    "subset_noteevents['Readmission_Prediction'] = predictions.tolist()\n",
    "\n",
    "# Assign back the predictions to the original DataFrame\n",
    "noteevents.loc[:999, 'Readmission_Prediction'] = subset_noteevents['Readmission_Prediction']\n",
    "\n",
    "# Check the predictions\n",
    "noteevents[['TEXT', 'Readmission_Prediction']].head(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch the model to evaluation mode\n",
    "model_biobert.eval()\n",
    "\n",
    "# Make predictions using the model\n",
    "with torch.no_grad():\n",
    "    outputs = model_biobert(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Convert predictions to a list for easier interpretation\n",
    "readmission_predictions = predictions.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            CLEAN_TEXT  Readmission_Prediction\n",
      "995  csru npn pt ao co pain recd percocet tab good ...                       0\n",
      "996  sinus bradycardia rate probable left ventricul...                       0\n",
      "997  cta chest wwo c recons ct non ionic contrast c...                       1\n",
      "998  ct chest wcontrast ct non ionic contrast clip ...                       1\n",
      "999  ct lungmediastinal bx ct guided needle plactme...                       1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjoth\\AppData\\Local\\Temp\\ipykernel_24044\\2582323298.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  noteevents_small['Readmission_Prediction'] = readmission_predictions\n"
     ]
    }
   ],
   "source": [
    "# Add predictions as a new column in your DataFrame\n",
    "noteevents_small['Readmission_Prediction'] = readmission_predictions\n",
    "\n",
    "# Display the first few rows to check the predictions\n",
    "print(noteevents_small[['CLEAN_TEXT', 'Readmission_Prediction']].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60140eaf77043e5955fa27b369488b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0\n",
      " 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0\n",
      " 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0\n",
      " 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# The predictions object contains logits, labels, and the metrics\n",
    "predicted_labels = predictions.predictions.argmax(axis=-1)  # Convert logits to class predictions\n",
    "\n",
    "# Optionally, compare predicted labels with actual labels\n",
    "actual_labels = predictions.label_ids\n",
    "\n",
    "print(predicted_labels)\n",
    "print(actual_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: After fine-tuning the model on our training dataset, we need to evaluate how well it performs on unseen data (the test dataset). This process is crucial for understanding how well the model generalizes to new examples.\n",
    "\n",
    "Prediction: By running trainer.predict(test_dataset), we're making predictions on the test data. This step is needed to see how the model performs in practice and to obtain predicted labels.\n",
    "\n",
    "Logits to Class Predictions: The models outputs (logits) are raw scores, not direct class labels. The argmax operation converts these logits into actual class predictions, making it possible to compare the predicted classes with the actual classes.\n",
    "\n",
    "Model Performance Comparison: Once you have both predicted labels and actual labels, you can calculate metrics (accuracy, F1 score, etc.) to quantitatively assess the model's performance. This comparison is essential to determine how well the model is classifying the test data.\n",
    "\n",
    "\n",
    "Predictions: The model's logits are converted into class predictions using argmax.\n",
    "Comparison: The predicted labels are compared with the actual labels to assess model accuracy and performance.\n",
    "Evaluation: This process is part of the model evaluation phase, where you assess how well the model generalizes to the test data.\n",
    "\n",
    "\n",
    "\n",
    "Key Observations:\n",
    "Predicted Labels (First Array): These are the labels predicted by the model after evaluating the test dataset. The values are either 0 or 1, where 0 may represent one class (e.g., a negative class) and 1 another class (e.g., a positive class).\n",
    "\n",
    "Actual Labels (Second Array): These are the true labels corresponding to the test dataset. They represent the correct classification for each instance, where 0 and 1 correspond to the same classes as in the predicted labels.\n",
    "\n",
    "Comparison Between Predicted and Actual Labels:\n",
    "Correct Predictions: When the predicted label matches the actual label, the model made a correct prediction. For example, at index 0, both the predicted and actual labels are 0, indicating a correct prediction. Similarly, at index 9, both predicted and actual labels are 1.\n",
    "\n",
    "Incorrect Predictions: When the predicted label differs from the actual label, the model made an incorrect prediction. For example, at index 8, the predicted label is 0, while the actual label is 1, indicating an error in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming `y_true` contains actual labels and `y_pred` contains predicted labels\n",
    "conf_matrix = confusion_matrix(y_true, predictions)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_biobert')\n",
    "model = BertForSequenceClassification.from_pretrained('./fine_tuned_biobert')\n",
    "\n",
    "# Select the texts starting from row 1001 to the end\n",
    "noteevents_test = noteevents['TEXT'][1000:2000]\n",
    "\n",
    "# Tokenize the new texts\n",
    "inputs = tokenizer(noteevents_test.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Print predictions\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>TEXT_LENGTH</th>\n",
       "      <th>CLEAN_TEXT</th>\n",
       "      <th>Readmission_Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1678764</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>17/07/2138</td>\n",
       "      <td>17/07/2138 22:51</td>\n",
       "      <td>17/07/2138 23:12</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16929.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neonatology Attending Triage Note\\n\\nBaby [**N...</td>\n",
       "      <td>1296</td>\n",
       "      <td>neonatology attending triage note baby term ma...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1678765</td>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>17/07/2138</td>\n",
       "      <td>17/07/2138 23:08</td>\n",
       "      <td>17/07/2138 23:18</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>17774.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nursing Transfer note\\n\\n\\nPt admitted to NICU...</td>\n",
       "      <td>522</td>\n",
       "      <td>nursing transfer note pt admitted nicu sepsis ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272794</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/10/2101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECG</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sinus rhythm\\nInferior/lateral ST-T changes ar...</td>\n",
       "      <td>162</td>\n",
       "      <td>sinus rhythm inferiorlateral stt changes nonsp...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>769224</td>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>26/10/2101</td>\n",
       "      <td>26/10/2101 06:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...</td>\n",
       "      <td>1310</td>\n",
       "      <td>chest portable ap clip reason ro infiltrate __...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>272793</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/10/2101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECG</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sinus rhythm\\nA-V delay\\nNonspecific inferior ...</td>\n",
       "      <td>129</td>\n",
       "      <td>sinus rhythm av delay nonspecific inferior wav...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROW_ID SUBJECT_ID HADM_ID   CHARTDATE         CHARTTIME         STORETIME  \\\n",
       "0  1678764          2  163353  17/07/2138  17/07/2138 22:51  17/07/2138 23:12   \n",
       "1  1678765          2  163353  17/07/2138  17/07/2138 23:08  17/07/2138 23:18   \n",
       "2   272794          3     NaN  06/10/2101               NaN               NaN   \n",
       "3   769224          3  145834  26/10/2101  26/10/2101 06:01               NaN   \n",
       "4   272793          3     NaN  11/10/2101               NaN               NaN   \n",
       "\n",
       "        CATEGORY          DESCRIPTION     CGID ISERROR  \\\n",
       "0  Nursing/other               Report  16929.0     NaN   \n",
       "1  Nursing/other               Report  17774.0     NaN   \n",
       "2            ECG               Report      NaN     NaN   \n",
       "3      Radiology  CHEST (PORTABLE AP)      NaN     NaN   \n",
       "4            ECG               Report      NaN     NaN   \n",
       "\n",
       "                                                TEXT  TEXT_LENGTH  \\\n",
       "0  Neonatology Attending Triage Note\\n\\nBaby [**N...         1296   \n",
       "1  Nursing Transfer note\\n\\n\\nPt admitted to NICU...          522   \n",
       "2  Sinus rhythm\\nInferior/lateral ST-T changes ar...          162   \n",
       "3  [**2101-10-26**] 6:01 AM\\n CHEST (PORTABLE AP)...         1310   \n",
       "4  Sinus rhythm\\nA-V delay\\nNonspecific inferior ...          129   \n",
       "\n",
       "                                          CLEAN_TEXT  Readmission_Prediction  \n",
       "0  neonatology attending triage note baby term ma...                     0.0  \n",
       "1  nursing transfer note pt admitted nicu sepsis ...                     0.0  \n",
       "2  sinus rhythm inferiorlateral stt changes nonsp...                     0.0  \n",
       "3  chest portable ap clip reason ro infiltrate __...                     0.0  \n",
       "4  sinus rhythm av delay nonspecific inferior wav...                     0.0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If noteevents_test is a Series and you need it as a DataFrame\n",
    "noteevents_test_df = noteevents_test.to_frame()\n",
    "\n",
    "# Then you can inspect the columns (only if it's a DataFrame)\n",
    "noteevents.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME',\n",
       "       'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT',\n",
       "       'TEXT_LENGTH', 'CLEAN_TEXT', 'Readmission_Prediction'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noteevents.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjoth\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_sanitize_parameters() got an unexpected keyword argument 'truncation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdmis-lab/biobert-base-cased-v1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize NER pipeline with truncation to handle long text\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m nlp_ner \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Apply NER to the 'TEXT' column and store the results in a new column\u001b[39;00m\n\u001b[0;32m     11\u001b[0m noteevents_small[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNER_Entities\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m noteevents_small[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: nlp_ner(x[:\u001b[38;5;241m512\u001b[39m]))  \u001b[38;5;66;03m# Truncate to 512 tokens\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\pipelines\\__init__.py:1097\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1095\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m-> 1097\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\pipelines\\token_classification.py:135\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__init__\u001b[1;34m(self, args_parser, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args_parser\u001b[38;5;241m=\u001b[39mTokenClassificationArgumentHandler(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[0;32m    137\u001b[0m         TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES\n\u001b[0;32m    140\u001b[0m     )\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_basic_tokenizer \u001b[38;5;241m=\u001b[39m BasicTokenizer(do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\pipelines\\base.py:919\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 919\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor, BaseImageProcessor):\n\u001b[0;32m    923\u001b[0m         \u001b[38;5;66;03m# Backward compatible change, if users called\u001b[39;00m\n\u001b[0;32m    924\u001b[0m         \u001b[38;5;66;03m# ImageSegmentationPipeline(.., feature_extractor=MyFeatureExtractor())\u001b[39;00m\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# then we should keep working\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: _sanitize_parameters() got an unexpected keyword argument 'truncation'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# Load pre-trained BioBERT model for NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "# Initialize NER pipeline with truncation to handle long text\n",
    "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, truncation=True, max_length=512)\n",
    "\n",
    "# Apply NER to the 'TEXT' column and store the results in a new column\n",
    "noteevents_small['NER_Entities'] = noteevents_small['TEXT'].apply(lambda x: nlp_ner(x[:512]))  # Truncate to 512 tokens\n",
    "\n",
    "# Display the results\n",
    "print(noteevents_small[['TEXT', 'NER_Entities']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating labels manually based on some criteria in the 'TEXT' column\n",
    "# (This is just an example, and the criteria should be defined based on your actual data and requirements)\n",
    "noteevents['LABEL'] = noteevents['TEXT'].apply(lambda x: 1 if 'readmission' in x.lower() else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'LABEL' is a column in your noteevents DataFrame that contains the true labels\n",
    "new_labels = noteevents['LABEL'][:1000].tolist()  # Extract the first 1000 true labels\n",
    "\n",
    "# Create binary labels based on whether the text contains the word \"readmission\"\n",
    "new_labels = [1 if 'readmission' in text.lower() else 0 for text in noteevents['TEXT'][:1000]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.789\n",
      "Precision: 0.0047169811320754715\n",
      "Recall: 1.0\n",
      "F1 Score: 0.009389671361502346\n",
      "AUC-ROC: 0.8943943943943944\n"
     ]
    }
   ],
   "source": [
    "# Assuming you are working with the first 1000 samples\n",
    "predictions_subset = predictions[:1000]  # Slice to match the length of new_labels\n",
    "\n",
    "# Now evaluate the model predictions\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "accuracy = accuracy_score(new_labels, predictions_subset)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(new_labels, predictions_subset, average='binary')\n",
    "auc = roc_auc_score(new_labels, predictions_subset)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'AUC-ROC: {auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 788, 1: 212}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(predictions_subset, return_counts=True)\n",
    "print(dict(zip(unique, counts)))  # Prints the counts of 0s and 1s in predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 999, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(new_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'X_train' is your feature matrix and 'y_train' are the labels\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Split the resampled data into train/test sets again\n",
    "X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Undersample the majority class\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Calculate class weights (adjust the values based on the class imbalance)\n",
    "class_weights = torch.tensor([0.1, 0.9]).to(device)  # You can adjust these values based on your dataset\n",
    "\n",
    "# Define the loss function with class weights\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # Move class_weights to the device the model is using\n",
    "        class_weights = torch.tensor([0.1, 0.9]).to(model.device)\n",
    "        loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer_biobert = WeightedTrainer(\n",
    "    model=model_biobert,\n",
    "    args=training_args_biobert,\n",
    "    train_dataset=train_dataset_biobert,\n",
    "    eval_dataset=test_dataset_biobert,\n",
    "    data_collator=data_collator_biobert,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# After you get the predicted probabilities (logits), apply a custom threshold\n",
    "threshold = 0.3  # Adjust this value based on your ROC curve analysis\n",
    "proba = model_biobert(**inputs).logits.softmax(dim=-1)  # Get probabilities\n",
    "predictions_custom_threshold = (proba[:, 1] > threshold).int()  # Apply the threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Define your model and inputs (tokenized test data)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "y_true = noteevents_small['LABEL'].tolist()  # Assuming 'LABEL' column contains true labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.7897897897897898\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming 'LABEL' column in noteevents contains the true labels\n",
    "#y_true = noteevents_small['LABEL'].tolist()  # Extract true labels\n",
    "y_true = new_labels\n",
    "# Tokenize the entire dataset\n",
    "inputs = tokenizer_biobert(noteevents_small['TEXT'].tolist(), \n",
    "                           return_tensors=\"pt\", \n",
    "                           truncation=True, \n",
    "                           padding=True, \n",
    "                           max_length=128)\n",
    "\n",
    "# Create a DataLoader to handle batching\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n",
    "dataloader = DataLoader(dataset, batch_size=32)  # Adjust batch_size as needed\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_biobert.eval()\n",
    "\n",
    "y_pred_proba = []\n",
    "\n",
    "# Get predictions from the model in batches\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask = batch\n",
    "        outputs = model_biobert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Convert logits to probabilities for class 1\n",
    "        probabilities = softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()\n",
    "        y_pred_proba.extend(probabilities)\n",
    "\n",
    "# Ensure the number of predictions matches the number of true labels\n",
    "assert len(y_true) == len(y_pred_proba), \"Mismatch in number of predictions and true labels!\"\n",
    "\n",
    "# Calculate AUC-ROC using true labels and predicted probabilities\n",
    "auc = roc_auc_score(y_true, y_pred_proba)\n",
    "print(f'AUC-ROC: {auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert your test data to tensors if not already done\n",
    "X_test_tensor = X_testy_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)  # You can adjust the batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dataloader:\n",
    "    # Convert batch[0] to a list of strings if necessary\n",
    "    text_data = [str(text) for text in batch[0]]\n",
    "    \n",
    "    inputs = tokenizer_biobert(text_data, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_biobert(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the type of batch[0] before tokenization\n",
    "print(type(batch[0]), batch[0][:2])  # print the first two samples of batch[0]\n",
    "\n",
    "# If batch[0] is already tokenized, extract raw text from original data before this point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    # Assuming batch[0] contains tokenized text data, convert it back to strings\n",
    "    texts = tokenizer_biobert.decode(batch[0], skip_special_tokens=True)\n",
    "    inputs = tokenizer_biobert(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_biobert(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(batch[1])\n",
    "\n",
    "y_true = np.array(all_labels)\n",
    "y_pred_proba = np.array(all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of y_true: 200\n",
      "Length of y_pred_proba: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of y_true: {len(y_true)}\")\n",
    "print(f\"Length of y_pred_proba: {len(y_pred_proba)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    # Assuming batch[0] contains tokenized text data, convert it back to strings\n",
    "    texts = [item.decode() if isinstance(item, bytes) else item for item in batch[0]]\n",
    "    inputs = tokenizer_biobert(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_biobert(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()\n",
    "\n",
    "    # Add debug statements to check the batch size\n",
    "    print(f\"Processing batch {i + 1}:\")\n",
    "    print(f\"Predictions: {len(predictions)}\")\n",
    "    print(f\"Labels: {len(batch[1])}\")\n",
    "\n",
    "    all_predictions.extend(predictions)\n",
    "    all_labels.extend(batch[1])\n",
    "\n",
    "y_true = np.array(all_labels)\n",
    "y_pred_proba = np.array(all_predictions)\n",
    "\n",
    "print(f\"Final length of y_true: {len(y_true)}\")\n",
    "print(f\"Final length of y_pred_proba: {len(y_pred_proba)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [200, 7]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate precision and recall\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m precision, recall, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_proba\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Plot the Precision-Recall curve\u001b[39;00m\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(recall, precision)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_ranking.py:952\u001b[0m, in \u001b[0;36mprecision_recall_curve\u001b[1;34m(y_true, probas_pred, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    853\u001b[0m     {\n\u001b[0;32m    854\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    863\u001b[0m     y_true, probas_pred, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    864\u001b[0m ):\n\u001b[0;32m    865\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision-recall pairs for different probability thresholds.\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \n\u001b[0;32m    867\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;124;03m    array([0.1 , 0.35, 0.4 , 0.8 ])\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 952\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobas_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    957\u001b[0m         \u001b[38;5;66;03m# Drop thresholds corresponding to points where true positives (tps)\u001b[39;00m\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;66;03m# do not change from the previous or subsequent point. This will keep\u001b[39;00m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;66;03m# only the first and last point for each tps value. All points\u001b[39;00m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;66;03m# with the same tps value have the same recall and thus x coordinate.\u001b[39;00m\n\u001b[0;32m    961\u001b[0m         \u001b[38;5;66;03m# They appear as a vertical line on the plot.\u001b[39;00m\n\u001b[0;32m    962\u001b[0m         optimal_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[0;32m    963\u001b[0m             np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m    964\u001b[0m                 [[\u001b[38;5;28;01mTrue\u001b[39;00m], np\u001b[38;5;241m.\u001b[39mlogical_or(np\u001b[38;5;241m.\u001b[39mdiff(tps[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), np\u001b[38;5;241m.\u001b[39mdiff(tps[\u001b[38;5;241m1\u001b[39m:])), [\u001b[38;5;28;01mTrue\u001b[39;00m]]\n\u001b[0;32m    965\u001b[0m             )\n\u001b[0;32m    966\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_ranking.py:806\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m--> 806\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    807\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n\u001b[0;32m    808\u001b[0m y_score \u001b[38;5;241m=\u001b[39m column_or_1d(y_score)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [200, 7]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "\n",
    "# Plot the Precision-Recall curve\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) are popular explainability techniques used to interpret machine learning models. Both of these methods provide insights into how models make predictions, and they can be used with transformers like BioBERT for explaining predictions in NLP tasks, such as Named Entity Recognition (NER) or patient readmission prediction.\n",
    "\n",
    "1. LIME:\n",
    "LIME explains individual predictions by creating simple surrogate models (e.g., linear models) around the prediction point and observing how small changes in the input affect the outcome. For text-based models, LIME can highlight specific words or tokens that contribute most to the model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Get an explanation for a single sample\u001b[39;00m\n\u001b[0;32m     16\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatient was readmitted within 30 days after the initial discharge.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m explanation \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Visualize the explanation\u001b[39;00m\n\u001b[0;32m     20\u001b[0m explanation\u001b[38;5;241m.\u001b[39mshow_in_notebook(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\lime\\lime_text.py:429\u001b[0m, in \u001b[0;36mLimeTextExplainer.explain_instance\u001b[1;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    425\u001b[0m     ret_exp\u001b[38;5;241m.\u001b[39mtop_labels\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[0;32m    427\u001b[0m     (ret_exp\u001b[38;5;241m.\u001b[39mintercept[label],\n\u001b[0;32m    428\u001b[0m      ret_exp\u001b[38;5;241m.\u001b[39mlocal_exp[label],\n\u001b[1;32m--> 429\u001b[0m      ret_exp\u001b[38;5;241m.\u001b[39mscore, ret_exp\u001b[38;5;241m.\u001b[39mlocal_pred) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance_with_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_regressor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_regressor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_selection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_selection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret_exp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\lime\\lime_base.py:182\u001b[0m, in \u001b[0;36mLimeBase.explain_instance_with_data\u001b[1;34m(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Takes perturbed data, labels and distances, returns explanation.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    local_pred is the prediction of the explanation model on the original instance\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    181\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_fn(distances)\n\u001b[1;32m--> 182\u001b[0m labels_column \u001b[38;5;241m=\u001b[39m \u001b[43mneighborhood_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    183\u001b[0m used_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_selection(neighborhood_data,\n\u001b[0;32m    184\u001b[0m                                        labels_column,\n\u001b[0;32m    185\u001b[0m                                        weights,\n\u001b[0;32m    186\u001b[0m                                        num_features,\n\u001b[0;32m    187\u001b[0m                                        feature_selection)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_regressor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the text classifier (assuming it's already trained)\n",
    "explainer = LimeTextExplainer(class_names=[\"No Readmission\", \"Readmission\"])\n",
    "\n",
    "# Use the tokenizer and model in a pipeline\n",
    "text_pipeline = pipeline(\"text-classification\", model=model_biobert, tokenizer=tokenizer_biobert)\n",
    "\n",
    "# Modify the predict function to return a 2D numpy array\n",
    "def predict_fn(texts):\n",
    "    predictions = text_pipeline(texts)\n",
    "    scores = np.array([[pred['score'], 1 - pred['score']] for pred in predictions])\n",
    "    return scores\n",
    "\n",
    "# Continue with LIME explanation\n",
    "sample_text = \"Patient was readmitted within 30 days after the initial discharge.\"\n",
    "explanation = explainer.explain_instance(sample_text, predict_fn, num_features=6)\n",
    "\n",
    "# Visualize the explanation\n",
    "explanation.show_in_notebook(text=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "\n",
    "# LIME expects the prediction function to return class probabilities, so we'll define a function for it.\n",
    "class_names = ['Not Readmitted', 'Readmitted']\n",
    "\n",
    "def predict_fn(texts):\n",
    "    # Tokenize the texts\n",
    "    inputs = tokenizer_biobert(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_biobert(**inputs)\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "# Initialize the LIME explainer for text data\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# Example text to explain\n",
    "example_text = noteevents[:1000]['TEXT'][0]  # Pick one text sample from your dataset\n",
    "\n",
    "# Generate the explanation\n",
    "exp = explainer.explain_instance(example_text, predict_fn, num_features=10)\n",
    "\n",
    "# Display the explanation in the console\n",
    "print(exp.as_list())\n",
    "\n",
    "# Visualize the explanation in a Jupyter notebook\n",
    "exp.show_in_notebook(text=example_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME breaks down the prediction into interpretable components (e.g., specific words) and shows which parts of the input contribute most to the prediction (whether a patient is likely to be readmitted or not).\n",
    "In this case, LIME will highlight important tokens that influenced the model's decision on the patient readmission prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SHAP:\n",
    "SHAP is based on cooperative game theory and aims to provide a global understanding of the model by assigning each feature an importance value. SHAP values can be applied to both local (individual prediction) and global (overall model behavior) interpretations.\n",
    "\n",
    "Use SHAP with Text Data: SHAP can also be used with text-based models like BioBERT. Below is an example that demonstrates how to explain a text classification model (like patient readmission prediction) with SHAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Explain predictions for a single sample or batch of samples\u001b[39;00m\n\u001b[0;32m     17\u001b[0m sample_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatient was readmitted due to complications.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 18\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Visualize the SHAP values for the explanation\u001b[39;00m\n\u001b[0;32m     21\u001b[0m shap\u001b[38;5;241m.\u001b[39mplots\u001b[38;5;241m.\u001b[39mtext(shap_values[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\shap\\explainers\\_permutation.py:79\u001b[0m, in \u001b[0;36mPermutationExplainer.__call__\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, main_effects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error_bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m              outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     77\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Explain the output of the model on the given arguments.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\shap\\explainers\\_explainer.py:267\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39margs), num_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent):\n\u001b[1;32m--> 267\u001b[0m     row_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    272\u001b[0m     output_indices\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\shap\\explainers\\_permutation.py:89\u001b[0m, in \u001b[0;36mPermutationExplainer.explain_row\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Explains a single row and returns the tuple (row_values, row_expected_values, row_mask_shapes).\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# build a masked version of the model for the current input sample\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m fm \u001b[38;5;241m=\u001b[39m \u001b[43mMaskedModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinearize_link\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# by default we run 10 permutations forward and backward\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_evals \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\shap\\utils\\_masked_model.py:50\u001b[0m, in \u001b[0;36mMaskedModel.__init__\u001b[1;34m(self, model, masker, link, linearize_link, *args)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masker_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;66;03m# # just assuming...\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masker_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linearizing_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\shap\\utils\\_masked_model.py:50\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masker_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;66;03m# # just assuming...\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masker_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39mprod(\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linearizing_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize SHAP explainer (use a pipeline that can return logits)\n",
    "text_pipeline = pipeline(\"text-classification\", model=model_biobert, tokenizer=tokenizer_biobert, return_all_scores=True)\n",
    "\n",
    "# Define a function for SHAP to use\n",
    "def predict_proba(texts):\n",
    "    results = text_pipeline(texts)\n",
    "    # Extract the scores from the predictions\n",
    "    return [[pred[1]['score'], pred[0]['score']] for pred in results]\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.Explainer(predict_proba, text_pipeline)\n",
    "\n",
    "# Explain predictions for a single sample or batch of samples\n",
    "sample_texts = [\"Patient was readmitted due to complications.\"]\n",
    "shap_values = explainer(sample_texts)\n",
    "\n",
    "# Visualize the SHAP values for the explanation\n",
    "shap.plots.text(shap_values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Between LIME and SHAP:\n",
    "LIME is more focused on local explanations and provides insights into individual predictions by training a surrogate model around a specific prediction. It is fast for generating single-instance explanations but may not give global insights.\n",
    "\n",
    "SHAP, on the other hand, is more robust for both local and global explanations and can offer a comprehensive view of how the model makes decisions across the entire dataset. It is more computationally intensive than LIME but provides deeper insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
